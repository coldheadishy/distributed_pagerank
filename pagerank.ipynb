{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"4yIKQ-LR5nBk"},"source":["## Distributed Page Rank\n","---"]},{"cell_type":"markdown","metadata":{"id":"vdL9NkD55nBo"},"source":["We have a graph structure where each node represents a server, and each (directed) edge represents a connection between servers, a text file represents the graph as an adjacency list. Stanford Network Analysis Project, collected these data, [the graph includes 6301 servers and 20777 edges](http://snap.stanford.edu/data/p2p-Gnutella08.html)."]},{"cell_type":"markdown","metadata":{"id":"x2xfj_3e5nBt"},"source":["---\n","Determining some basic properties of the graph:\n","- \\# of nodes and edges\n","- \\# of nodes of each outdegree\n","- \\# of nodes of each indegree\n","\n","We use Spark to determine this."]},{"cell_type":"code","metadata":{"id":"TnoJMCQY5nBu"},"source":["def nodes_edges():\n","    num_nodes = sc.accumulator(0) # initialize count to 0\n","    num_edges = sc.accumulator(0) # initialize to 0\n","    lines = sc.textFile(\"_.txt\") # load file\n","    '''\n","    REDACTED\n","    '''\n","    lines.foreach(lambda x: num_edges.add(\n","        len(x.split()) - 1)) # add number of space delimited items on line - 1 to count\n","    return (num_nodes.value, num_edges.value) # extract values\n","\n","def out_dict():\n","    '''\n","    REDACTED\n","    '''\n","    outdegrees = lines.map(lambda x: (len(x.split()) - 1, 1)).reduceByKey(\n","        lambda x, y: x + y) # count number of neighbours on each line and take sum\n","    return outdegrees.sortByKey(True).collectAsMap() # sort by node id\n","\n","def in_counts():\n","    lines = sc.textFile(\"_.txt\") # load file\n","    '''\n","    REDACTED\n","    '''\n","    indegrees = neighbours.reduceByKey(lambda x, y: x + y).map(\n","        lambda x: (x[1], 1)).reduceByKey(\n","            lambda x, y: x + y) # for each node_id, count number of occurrences\n","                                #   as a neighbour\n","    return indegrees.sortByKey(True).collectAsMap() # sort by node_id\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jNp6bYDY5nBv"},"source":["---\n","This Spark program performs page rank with respect to a soruce node."]},{"cell_type":"code","metadata":{"id":"7oo6ny035nBv"},"source":["def page_rank(source_id, n_iter, jump, m):\n","    lines = sc.textFile(\"_.txt\") # load file\n","    links = lines.map(lambda x: list(map(\n","        int, x.split()))).map(\n","            lambda x: (x[0], x[1:])) # create key value pairs of nodes and\n","                                     #    neighbour list\n","\n","    # calc_contrib(node_id, adj, rank) determines how to distribute mass given\n","    #   by rank to the neighbours of node_id given by adj\n","    def calc_contrib(node_id, adj, rank):\n","        num_neighbours = len(adj)\n","        if num_neighbours == 0: # detect dead end\n","            lost_mass = lost_mass_curr + rank # accumulate lost mass\n","        '''\n","        REDACTED\n","        '''\n","        return vec\n","\n","    ranks = links.map(lambda x:\n","                      (x[0], 1.0 if x[0] == source_id else 0.0)) # set up p0\n","\n","    '''\n","    REDACTED\n","    '''\n","\n","    for i in range(0, n_iter):\n","        '''\n","        REDACTED\n","        '''\n","        contribs = links.join(ranks).flatMap(\n","            lambda x: calc_contrib(\n","                x[0], x[1][0], x[1][1])) # perform page rank step\n","        ranks = contribs.reduceByKey(lambda x, y: x + y).sortByKey().map(\n","            lambda x: (x[0], x[1] * (1-jump_factor) +\n","                       ((jump_factor + lost_mass_prev)\n","                       if x[0] == source_id else 0.0))) # compute p[i+1]\n","        '''\n","        REDACTED\n","        '''\n","\n","    output = ranks.sortBy(lambda x: x[1], False).take(m) # sort by probability\n","    return output\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RmZAajwV5nBw"},"source":["---\n","We perform page rank here until we reach a satisfiably close to steady state (steady state as defined in the context of a Markov Chain represented as a stochastic matrix).\n","\n","We cache an RDD here as it is used repeatedly in the loop during the join operation. This is after considering the tradeoff between storing it on disk versus repeatedly computing it during each iteration, the former seems more suitable as we tend to write spark code following a loosely functional paradigm and use this RDD many times for further processing. Caching it ensures it is preserved and reused."]},{"cell_type":"code","metadata":{"id":"whRf8BnZ5nBx"},"source":["def page_rank_criterion(source_id, jump):\n","    lines = sc.textFile(\"_.txt\") # load file\n","    num_nodes = sc.accumulator(0) # initialize to 0\n","    '''\n","    REDACTED\n","    '''\n","\n","    links = lines.map(lambda x: list(map(\n","        int, x.split()))).map(lambda x: (x[0], x[1:])) # create key value pairs of nodes and\n","                                                       #    neighbour list\n","\n","    # calc_contrib(node_id, adj, rank) determines how to distribute mass given\n","    #   by rank to the neighbours of node_id given by adj\n","    def calc_contrib(node_id, adj, rank):\n","        '''\n","        REDACTED\n","        '''\n","\n","    def det_jump(node_id):\n","        '''\n","        REDACTED\n","        '''\n","\n","    ranks_prev = links.map(\n","        lambda x: (x[0], 1.0 if x[0] == source_node_id else 0.0)) # set up p0\n","\n","    '''\n","    REDACTED\n","    '''\n","\n","    while(change > threshold):\n","        '''\n","        REDACTED\n","        '''\n","        contribs = links.join(ranks_prev).flatMap(\n","            lambda x: calc_contrib(\n","                x[0], x[1][0], x[1][1])) # perform page rank step\n","        ranks_curr = contribs.reduceByKey(lambda x, y: x + y).sortByKey().map(\n","            lambda x: (x[0], x[1] * (1-jump_factor) +\n","                       ((jump_factor + lost_mass_prev)\n","                       if x[0] == source_node_id else 0.0))) # compute p[i+1]\n","        '''\n","        REDACTED\n","        '''\n","\n","    output = ranks_curr.sortBy(lambda x: x[1], False).take(10) # sort by probability\n","    return output\n"],"execution_count":null,"outputs":[]}]}